import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
from dotenv import load_dotenv
import os
import time
import csv
import json
from datetime import datetime
import re

# --- üîê C·∫§U H√åNH ---
# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# L·∫•y API keys t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Ki·ªÉm tra API keys ngay t·ª´ ƒë·∫ßu
if not all([GOOGLE_API_KEY, SEARCH_ENGINE_ID, GEMINI_API_KEY]):
    print("[ERROR] L·ªói nghi√™m tr·ªçng: Kh√¥ng t√¨m th·∫•y ƒë·ªß API Keys.")
    print("        Vui l√≤ng ki·ªÉm tra l·∫°i file .env c·ªßa b·∫°n ph·∫£i c√≥ ƒë·ªß:")
    print("        - GOOGLE_API_KEY")
    print("        - SEARCH_ENGINE_ID")
    print("        - GEMINI_API_KEY")
    exit()

# C·∫•u h√¨nh Gemini
try:
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel(model_name="gemini-1.5-flash-latest")
    print("[OK] C·∫•u h√¨nh v√† k·∫øt n·ªëi t·ªõi Gemini th√†nh c√¥ng.")
except Exception as e:
    print(f"[ERROR] L·ªói c·∫•u h√¨nh Gemini API: {e}")
    print("        Vui l√≤ng ki·ªÉm tra l·∫°i GEMINI_API_KEY.")
    exit()


# --- üåê C√ÅC H√ÄM H·ªñ TR·ª¢ ---

def search_google_for_urls(query, num_results=5):
    """
    S·ª≠ d·ª•ng Google Custom Search API ƒë·ªÉ t√¨m ki·∫øm v√† tr·∫£ v·ªÅ danh s√°ch c√°c URL.
    """
    print(f"  [SEARCH] T√¨m ki·∫øm Google v·ªõi truy v·∫•n: '{query}'")
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "q": query,
        "key": GOOGLE_API_KEY,
        "cx": SEARCH_ENGINE_ID,
        "num": num_results,
    }
    try:
        res = requests.get(url, params=params, timeout=10)
        res.raise_for_status()
        results = res.json().get("items", [])
        return [item["link"] for item in results]
    except requests.exceptions.RequestException as e:
        print(f"  [WARN] L·ªói khi g·ªçi Google Search API: {e}")
        return []

def scrape_website_content(url):
    """
    T·∫£i v√† tr√≠ch xu·∫•t n·ªôi dung vƒÉn b·∫£n t·ª´ m·ªôt URL.
    """
    print(f"  [SCRAPE] ƒêang qu√©t n·ªôi dung t·ª´: {url}")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        for script_or_style in soup(["script", "style", "header", "footer", "nav", "aside"]):
            script_or_style.decompose()
            
        text = soup.get_text(separator="\n", strip=True)
        return text
    except Exception as e:
        print(f"  [WARN] Kh√¥ng th·ªÉ qu√©t n·ªôi dung t·ª´ {url}: {e}")
        return ""

def _call_gemini_with_retry(prompt):
    """
    H√†m n·ªôi b·ªô ƒë·ªÉ g·ªçi Gemini, c√≥ c∆° ch·∫ø t·ª± ƒë·ªông th·ª≠ l·∫°i khi g·∫∑p l·ªói gi·ªõi h·∫°n.
    """
    max_retries = 3
    delay_seconds = 60 # Th·ªùi gian ch·ªù ban ƒë·∫ßu l√† 60 gi√¢y

    for attempt in range(max_retries):
        try:
            response = gemini_model.generate_content(prompt)
            # D·ªçn d·∫πp ph·∫£n h·ªìi ƒë·ªÉ ƒë·∫£m b·∫£o n√≥ l√† JSON h·ª£p l·ªá
            cleaned_response = response.text.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response[:-3]
            
            return json.loads(cleaned_response)
        except Exception as e:
            if "429" in str(e): # L·ªói gi·ªõi h·∫°n truy c·∫≠p
                print(f"  [WARN] L·ªói gi·ªõi h·∫°n truy c·∫≠p (429). ƒêang ch·ªù {delay_seconds} gi√¢y ƒë·ªÉ th·ª≠ l·∫°i... (L·∫ßn {attempt + 1}/{max_retries})")
                time.sleep(delay_seconds)
                delay_seconds *= 2 
            elif isinstance(e, json.JSONDecodeError):
                print("  [WARN] Gemini tr·∫£ v·ªÅ kh√¥ng ph·∫£i ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá.")
                return {"error": "L·ªói ph√¢n t√≠ch JSON t·ª´ Gemini.", "raw_response": response.text if 'response' in locals() else 'Kh√¥ng c√≥ ph·∫£n h·ªìi'}
            else:
                print(f"  [WARN] L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi Gemini API: {e}")
                return {"error": f"L·ªói Gemini kh√¥ng x√°c ƒë·ªãnh: {e}"}

    print("  [ERROR] V·∫´n g·∫∑p l·ªói gi·ªõi h·∫°n sau nhi·ªÅu l·∫ßn th·ª≠. Vui l√≤ng th·ª≠ l·∫°i sau.")
    return {"error": "H·∫øt gi·ªõi h·∫°n truy c·∫≠p API sau nhi·ªÅu l·∫ßn th·ª≠."}


def analyze_scraped_content_with_gemini(company_name, context):
    """
    Ph√¢n t√≠ch n·ªôi dung ƒë√£ qu√©t t·ª´ web.
    """
    print("  [ANALYZE 1/2] ƒê∆∞a d·ªØ li·ªáu web-scrape cho Gemini ph√¢n t√≠ch...")
    if not context.strip():
        return {"error": "Kh√¥ng c√≥ n·ªôi dung web-scrape ƒë·ªÉ ph√¢n t√≠ch."}

    prompt = f"""
    B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√¢n t√≠ch kinh doanh. D·ª±a v√†o kh·ªëi vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y v·ªÅ c√¥ng ty '{company_name}', h√£y tr√≠ch xu·∫•t th√¥ng tin v√† t√≥m t·∫Øt theo ƒë·ªãnh d·∫°ng JSON sau.
    H√£y d·ªçn d·∫πp v√† lo·∫°i b·ªè c√°c gi√° tr·ªã tr√πng l·∫∑p.
    
    C·∫•u tr√∫c JSON:
    {{
        "ten_giam_doc": ["T√™n 1", "T√™n 2"],
        "dia_chi": ["ƒê·ªãa ch·ªâ 1", "ƒê·ªãa ch·ªâ 2"],
        "mo_hinh_kd": ["Ng√†nh ngh·ªÅ kinh doanh ho·∫∑c s·∫£n ph·∫©m/d·ªãch v·ª• c·ª• th·ªÉ 1", "D·ªãch v·ª• 2"],
        "email": ["email1@example.com"],
        "ten_mien": ["domain1.com"],
        "tom_tat": "B·∫£n t√≥m t·∫Øt chi ti·∫øt v·ªÅ c√¥ng ty d·ª±a tr√™n n·ªôi dung ƒë∆∞·ª£c cung c·∫•p..."
    }}

    --- N·ªòI DUNG VƒÇN B·∫¢N (t·ª´ web scrape) ---
    {context}
    --- K·∫æT TH√öC N·ªòI DUNG ---
    """
    return _call_gemini_with_retry(prompt)

# <<< M·ªöI: H√†m ƒë·ªÉ l·∫•y th√¥ng tin tr·ª±c ti·∫øp t·ª´ ki·∫øn th·ª©c c·ªßa Gemini >>>
def get_info_directly_from_gemini(company_name):
    """
    H·ªèi tr·ª±c ti·∫øp Gemini v·ªÅ th√¥ng tin c·ªßa c√¥ng ty.
    """
    print(f"  [ANALYZE 2/2] H·ªèi tr·ª±c ti·∫øp Gemini v·ªÅ '{company_name}'...")

    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n gia thu th·∫≠p th√¥ng tin doanh nghi·ªáp. D·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n, h√£y cung c·∫•p th√¥ng tin v·ªÅ c√¥ng ty c√≥ t√™n l√† '{company_name}'.
    H√£y tr·∫£ l·ªùi b·∫±ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON duy nh·∫•t theo c·∫•u tr√∫c sau.
    
    C·∫•u tr√∫c JSON:
    {{
        "ten_giam_doc": ["T√™n gi√°m ƒë·ªëc ho·∫∑c ng∆∞·ªùi ƒë·∫°i di·ªán"],
        "dia_chi": ["ƒê·ªãa ch·ªâ tr·ª• s·ªü ch√≠nh ho·∫∑c c√°c chi nh√°nh n·ªïi b·∫≠t"],
        "mo_hinh_kd": ["C√°c ng√†nh ngh·ªÅ kinh doanh, s·∫£n ph·∫©m, d·ªãch v·ª• ch√≠nh"],
        "email": ["Email li√™n h·ªá ch√≠nh"],
        "ten_mien": ["T√™n mi·ªÅn website ch√≠nh th·ª©c"],
        "tom_tat": "M·ªôt b·∫£n t√≥m t·∫Øt t·ªïng quan v·ªÅ c√¥ng ty, l·ªãch s·ª≠, quy m√¥ v√† lƒ©nh v·ª±c ho·∫°t ƒë·ªông."
    }}
    """
    return _call_gemini_with_retry(prompt)

# <<< M·ªöI: H√†m ƒë·ªÉ t·ªïng h·ª£p d·ªØ li·ªáu t·ª´ hai ngu·ªìn >>>
def merge_data(scraped_data, direct_data):
    """
    Tr·ªôn d·ªØ li·ªáu t·ª´ k·∫øt qu·∫£ web-scrape v√† k·∫øt qu·∫£ h·ªèi tr·ª±c ti·∫øp Gemini.
    """
    print("  [MERGE] ƒêang t·ªïng h·ª£p d·ªØ li·ªáu t·ª´ 2 ngu·ªìn...")
    if "error" in scraped_data: scraped_data = {}
    if "error" in direct_data: direct_data = {}

    merged = {}
    
    # C√°c tr∆∞·ªùng d·∫°ng danh s√°ch (list)
    list_keys = ["ten_giam_doc", "dia_chi", "mo_hinh_kd", "email", "ten_mien"]
    for key in list_keys:
        # S·ª≠ d·ª•ng set ƒë·ªÉ t·ª± ƒë·ªông lo·∫°i b·ªè tr√πng l·∫∑p
        combined_set = set(scraped_data.get(key, []))
        combined_set.update(direct_data.get(key, []))
        # Chuy·ªÉn l·∫°i th√†nh list, lo·∫°i b·ªè c√°c gi√° tr·ªã r·ªóng n·∫øu c√≥
        merged[key] = sorted([item for item in list(combined_set) if item])

    # X·ª≠ l√Ω ph·∫ßn t√≥m t·∫Øt
    summary_scraped = scraped_data.get("tom_tat", "").strip()
    summary_direct = direct_data.get("tom_tat", "").strip()
    
    final_summary = ""
    if summary_scraped:
        final_summary += f"--- T√ìM T·∫ÆT T·ª™ PH√ÇN T√çCH WEB ---\n{summary_scraped}\n\n"
    if summary_direct:
        final_summary += f"--- T√ìM T·∫ÆT T·ª™ KI·∫æN TH·ª®C GEMINI ---\n{summary_direct}"
        
    merged["tom_tat"] = final_summary.strip() or "Kh√¥ng t·∫°o ƒë∆∞·ª£c t√≥m t·∫Øt."

    return merged


def save_data_to_csv(company_name, data_row):
    """
    L∆∞u m·ªôt d√≤ng d·ªØ li·ªáu v√†o file CSV ri√™ng bi·ªát cho c√¥ng ty.
    """
    safe_filename = re.sub(r'[\\/*?:"<>|]', "", company_name)
    safe_filename = safe_filename.replace(' ', '_')
    output_filename = f"{safe_filename}.csv"

    print(f"  [SAVE] ƒêang l∆∞u d·ªØ li·ªáu v√†o file: {output_filename}")
    try:
        with open(output_filename, "w", newline="", encoding="utf-8-sig") as file:
            writer = csv.writer(file)
            writer.writerow([
                "T√™n c√¥ng ty", "T√≥m t·∫Øt t·ªïng h·ª£p", "T√™n Gi√°m ƒë·ªëc", "ƒê·ªãa ch·ªâ/Chi nh√°nh",
                "M√¥ h√¨nh KD", "Email", "T√™n mi·ªÅn", "Th·ªùi gian qu√©t"
            ])
            writer.writerow(data_row)
        print(f"[OK] ƒê√£ l∆∞u th√†nh c√¥ng file '{output_filename}'")
    except IOError as e:
        print(f"[ERROR] Kh√¥ng th·ªÉ ghi v√†o file {output_filename}: {e}")


# --- üöÄ H√ÄM CH√çNH (ƒê√£ c·∫≠p nh·∫≠t quy tr√¨nh) ---

def process_company(company_name):
    """
    Th·ª±c hi·ªán to√†n b·ªô quy tr√¨nh cho m·ªôt c√¥ng ty: t√¨m ki·∫øm, qu√©t, ph√¢n t√≠ch v√† l∆∞u.
    """
    print(f"\n{'='*20} ƒêANG X·ª¨ L√ù: {company_name.upper()} {'='*20}")

    # === B∆Ø·ªöC 1: THU TH·∫¨P D·ªÆ LI·ªÜU T·ª™ WEB ===
    queries = [
        f'"{company_name}" ƒë·ªãa ch·ªâ chi nh√°nh',
        f'"{company_name}" h·ªá th·ªëng c·ª≠a h√†ng',
        f'"{company_name}" ng√†nh ngh·ªÅ kinh doanh',
        f'"{company_name}" gi·ªõi thi·ªáu c√¥ng ty',
        f'"{company_name}" masothue.com',
        f'"{company_name}" linkedin',
    ]
    all_urls = []
    for q in queries:
        all_urls.extend(search_google_for_urls(q, num_results=2))
        time.sleep(1)

    unique_urls = list(dict.fromkeys(all_urls))
    print(f"  [INFO] T√¨m th·∫•y t·ªïng c·ªông {len(unique_urls)} URL ƒë·ªôc nh·∫•t.")

    full_context = ""
    for url in unique_urls:
        content = scrape_website_content(url)
        if content:
            full_context += f"\n\n--- Ngu·ªìn: {url} ---\n{content}"
        time.sleep(1)

    # === B∆Ø·ªöC 2: PH√ÇN T√çCH D·ªÆ LI·ªÜU B·∫∞NG HAI LU·ªíNG ===
    scraped_analysis_result = analyze_scraped_content_with_gemini(company_name, full_context)
    direct_analysis_result = get_info_directly_from_gemini(company_name)

    # === B∆Ø·ªöC 3: T·ªîNG H·ª¢P K·∫æT QU·∫¢ ===
    if "error" in scraped_analysis_result and "error" in direct_analysis_result:
        print(f"[ERROR] C·∫£ hai lu·ªìng ph√¢n t√≠ch ƒë·ªÅu th·∫•t b·∫°i cho '{company_name}'.")
        error_message = f"L·ªói Scrape: {scraped_analysis_result.get('error', 'N/A')}\nL·ªói Direct: {direct_analysis_result.get('error', 'N/A')}"
        data_row = [company_name, error_message, "", "", "", "", "", datetime.now().strftime("%Y-%m-%d %H:%M:%S")]
        save_data_to_csv(company_name, data_row)
        return
        
    final_data = merge_data(scraped_analysis_result, direct_analysis_result)

    # === B∆Ø·ªöC 4: L∆ØU V√ÄO CSV ===
    separator = "\n" # D√πng newline ƒë·ªÉ ngƒÉn c√°ch c√°c gi√° tr·ªã trong m·ªôt √¥
    ceo = separator.join(final_data.get("ten_giam_doc", []))
    address = separator.join(final_data.get("dia_chi", []))
    model = separator.join(final_data.get("mo_hinh_kd", []))
    email = separator.join(final_data.get("email", []))
    domain = separator.join(final_data.get("ten_mien", []))
    summary = final_data.get("tom_tat", "Kh√¥ng c√≥ t√≥m t·∫Øt.")

    data_row = [
        company_name, summary, ceo or "Ch∆∞a r√µ", address or "Kh√¥ng c√≥",
        model or "Kh√¥ng r√µ", email or "Kh√¥ng t√¨m th·∫•y", domain or "Kh√¥ng c√≥",
        datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    ]
    
    save_data_to_csv(company_name, data_row)


def run_manual_mode():
    """
    Ch·∫°y ch∆∞∆°ng tr√¨nh ·ªü ch·∫ø ƒë·ªô nh·∫≠p tay.
    """
    print("\n--- CH∆Ø∆†NG TR√åNH T√åM KI·∫æM TH√îNG TIN DOANH NGHI·ªÜP (v2.0 - T·ªîNG H·ª¢P) ---")
    print("M·ªói c√¥ng ty s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω qua 2 lu·ªìng v√† l∆∞u v√†o m·ªôt file CSV ri√™ng bi·ªát.")
            
    while True:
        company_name = input("\n>>> Nh·∫≠p t√™n c√¥ng ty c·∫ßn t√¨m (ho·∫∑c g√µ 'exit' ƒë·ªÉ tho√°t): ").strip()
        if company_name.lower() == 'exit':
            break
        if company_name:
            process_company(company_name)

    print("\nCh∆∞∆°ng tr√¨nh ƒë√£ k·∫øt th√∫c.")


if __name__ == "__main__":
    run_manual_mode()